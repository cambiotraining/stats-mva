{
  "hash": "4058e34d9c2f2740edbfa2b9916d27f4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"K-means clustering\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip}\n## Learning outcomes\n\n- Understand how k-means clustering works\n- Be able to perform k-means clustering\n- Be able to optimise cluster number\n\n:::\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n### Libraries\n### Functions\n\n## Python\n\n### Libraries\n### Functions\n:::\n:::\n\n## Purpose and aim\n\nThis is a method for grouping observations into clusters. It groups data based on similarity and is an often-used unsupervised machine learning algorithm.\n\nIt groups the data into a fixed number of clusters ($k$) and the ultimate aim is to discover patterns in the data.\n\nK-means clustering is an iterative process. It follows the following steps:\n\n1. Select the number of clusters to identify (e.g. K = 3)\n2. Create centroids\n3. Place centroids randomly in your data\n4. Assign each data point to the closest centroid\n5. Calculate the centroid of each new cluster\n6. Repeat steps 4-5 until the clusters do not change\n\n## Data\n\nOnce again we'll be using the data from `data/penguins.csv`. These data are from the `palmerpenguins` package (for more information, see [the GitHub page](https://github.com/allisonhorst/palmerpenguins)).\n\n## Load and visualise the data\n\nIf you haven't done so already, load the data.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins <- read_csv(\"data/penguins.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 344 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl>\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex <chr>, year <dbl>\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# load the data\npenguins_py = pd.read_csv(\"data/penguins.csv\")\n\n# inspect the data\npenguins_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...       3750.0    male  2007\n1  Adelie  Torgersen            39.5  ...       3800.0  female  2007\n2  Adelie  Torgersen            40.3  ...       3250.0  female  2007\n3  Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n4  Adelie  Torgersen            36.7  ...       3450.0  female  2007\n\n[5 rows x 8 columns]\n```\n\n\n:::\n:::\n\n\n:::\n\nThere are a variety of variables in this data set. The following example focuses on the two variables `bill_length_mm` and `bill_depth_mm` across the various `species` recorded.,\n\nThese variables are distributed as follows:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(penguins, aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     colour = species)) +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](mva-kmeans_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(penguins_py,\n    aes(x = \"bill_depth_mm\",\n        y = \"bill_length_mm\",\n        colour = \"species\")) +\n    geom_point())\n```\n\n::: {.cell-output-display}\n![](mva-kmeans_files/figure-html/unnamed-chunk-6-1.png){width=614}\n:::\n:::\n\n\n:::\n\nWe can already see that the data appear to cluster quite closely by `species.` A great example to illustrate K-means clustering (you’d almost think I chose the example on purpose)!\n\n## Perform K-means clustering\n\nTo do the clustering, we’ll need to do a bit of data wrangling, since we can only do the clustering on numerical data.\n\nAs we did with the PCA, we also need to scale the data. Although it is not required in this case, because both variables have the same unit (millimetres), it is good practice. In other scenarios it could be that there are different units that are being compared, which could affect the clustering.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe're using the `kmeans()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_scaled <- penguins %>% \n  select(bill_depth_mm,          # select data\n         bill_length_mm) %>% \n  drop_na() %>%                  # remove missing values\n  scale() %>%                    # scale the data\n  as_tibble() %>% \n  rename(bill_depth_scaled = bill_depth_mm,\n         bill_length_scaled = bill_length_mm)\n```\n:::\n\n\nNext, we can perform the clustering:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkclust <- kmeans(penguins_scaled, # perform k-means clustering\n                 centers = 3)     # using 3 centers\n\nsummary(kclust)                  # summarise output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Length Class  Mode   \ncluster      342    -none- numeric\ncenters        6    -none- numeric\ntotss          1    -none- numeric\nwithinss       3    -none- numeric\ntot.withinss   1    -none- numeric\nbetweenss      1    -none- numeric\nsize           3    -none- numeric\niter           1    -none- numeric\nifault         1    -none- numeric\n```\n\n\n:::\n:::\n\n\nThe output is a list of vectors, with differing lengths. That's because they contain different types of information:\n\n* `cluster` contains information about each point\n* `centers`, `withinss`, and `size` contain information about each cluster\n* `totss`, `tot.withinss`, `betweenss`, and `iter` contain information about the full clustering\n\n## Python\n\nTo do the clustering, we'll be using the `KMeans()` function.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\n\n# remove missing values\npenguins_scaled_py = penguins_py.dropna()\n# select relevant columns\npenguins_scaled_py = penguins_scaled_py[['bill_depth_mm', 'bill_length_mm']]\n\npenguins_scaled_py = std_scaler.fit_transform(penguins_scaled_py)\n\nkmeans = KMeans(\ninit = 'random',\nn_clusters = 3,\nn_init = 10,\nmax_iter = 300,\nrandom_state = 42\n)\n\nkmeans.fit(penguins_scaled_py)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKMeans(init='random', n_clusters=3, random_state=42)\n```\n\n\n:::\n:::\n\n\n:::\n\n### Visualise clusters\n\nWhen we performed the clustering, the centers were calculated. These values give the (x, y) coordinates of the centroids.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_clust <- tidy(kclust) # get centroid coordinates\n\ntidy_clust\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  bill_depth_scaled bill_length_scaled  size withinss cluster\n              <dbl>              <dbl> <int>    <dbl> <fct>  \n1             0.799              1.10     64     39.0 1      \n2            -1.09               0.590   125     59.4 2      \n3             0.560             -0.943   153     88.0 3      \n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# calculate the cluster centers\nkclusts_py = kmeans.cluster_centers_\nkclusts_py = pd.DataFrame(kclusts_py, columns = ['0', '1'])\n\n# convert to Pandas DataFrame and rename columns\nkclusts_py = pd.DataFrame(kclusts_py)\n\nkclusts_py = (kclusts_py\n              .rename(columns = {\"0\": \"bdepth_scaled\",\n                                 \"1\": \"blength_scaled\"}))\n\n# and show the coordinates\nkclusts_py\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   bdepth_scaled  blength_scaled\n0      -1.098055        0.586444\n1       0.795036        1.088684\n2       0.553935       -0.950240\n```\n\n\n:::\n:::\n\n\n:::\n\n:::{.callout-note}\n## Initial centroid placement\n\nThe initial centroids get randomly placed in the data. This, combined with the iterative nature of the process, means that the values that you will see are going to be slightly different from the values here. That's normal!\n:::\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nNext, we want to visualise to which data points belong to which cluster. We can do that as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkclust %>%                              # take clustering data\n  augment(penguins_scaled) %>%          # combine with original data\n  ggplot(aes(x = bill_depth_scaled,     # plot the scaled data\n             y = bill_length_scaled)) +\n  geom_point(aes(colour = .cluster)) +  # colour by classification\n  geom_point(data = tidy_clust,\n             size = 7, shape = 'x')     # add the cluster centers\n```\n\n::: {.cell-output-display}\n![](mva-kmeans_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Python\n\nWe reformat and rename the scaled data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# convert NumPy arrays to Pandas DataFrame\npenguins_scaled_py = pd.DataFrame(penguins_scaled_py)\n\npenguins_scaled_py = penguins_scaled_py.rename(columns={0: 'bdepth_scaled', 1: 'blength_scaled'})\n```\n:::\n\n\n\nand merge this with the original data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# remove missing values\npenguins_py = penguins_py.dropna()\n# add an ID column\npenguins_py['id'] = range(1, len(penguins_py) + 1)\n\n# add an ID column to the scaled data\n# so we can match the observations\npenguins_scaled_py['id'] = range(1, len(penguins_scaled_py) + 1)\n\n# merge the data by ID\npenguins_augment_py = pd.merge(penguins_py.dropna(), penguins_scaled_py, on = 'id')\n\n# add the cluster designation\npenguins_augment_py['cluster'] = kmeans.fit_predict(penguins_scaled_py)\n\n# and convert it into a factor\npenguins_augment_py['cluster'] = penguins_augment_py['cluster'].astype('category')\n```\n:::\n\n\nWe can then (finally!) plot this:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(penguins_augment_py,\n       aes(x = 'bill_depth_mm',\n           y = 'bill_length_mm',\n           colour = 'cluster')) +\n           geom_point())\n```\n\n::: {.cell-output-display}\n![](mva-kmeans_files/figure-html/unnamed-chunk-15-1.png){width=614}\n:::\n:::\n\n\n:::\n\n## Optimising cluster number\n\nIn the example we set the number of clusters to 3. This made sense, because the data already visually separated in roughly three groups - one for each species.\n\nHowever, it might be that the cluster number to choose is a lot less obvious. In that case it would be helpful to explore clustering your data into a range of clusters.\n\nIn short, we determine which values of $k$ we want to explore and then loop through these values, repeating the workflow we looked at previously.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nReiterating over a range of $k$ values is reasonably straightforward using tidyverse. Although we could write our own function to loop through these $k$ values, tidyverse has a series of `map()` functions that can do this for you. More information on them [here](https://purrr.tidyverse.org/reference/map.html).\n\nIn short, the `map()` function spits out a [list](https://www.w3schools.com/r/r_lists.asp) which contains the output. When we do this on our data, we can create a table that contains lists with all of the information that we need.\n\nHere we calculate the following:\n\n1. the `kclust` column contains a list with all the `kmeans()` output, for each value of $k$\n2. the `tidied` column contains the information on a per-cluster basis\n3. the `glanced` column contains single-row summary for each $k$ - we'll use the `tot.withinss` values a little bit later on\n4. the `augmented` column contains the original data, augmented with the classification that was calculated by the `kmeans()` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkclusts <- \n  # check for k = 1 to 6\n  tibble(k = 1:6) %>%\n  mutate(\n    # perform clustering for each k\n    kclust = map(k, ~ kmeans(penguins_scaled, .x)),\n    # summary at per-cluster level\n    tidied = map(kclust, tidy),\n    # get single-row summary\n    glanced = map(kclust, glance),\n    # add classification to data set\n    augmented = map(kclust, augment, penguins_scaled))\n\nkclusts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n      k kclust   tidied           glanced          augmented         \n  <int> <list>   <list>           <list>           <list>            \n1     1 <kmeans> <tibble [1 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n2     2 <kmeans> <tibble [2 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n3     3 <kmeans> <tibble [3 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n4     4 <kmeans> <tibble [4 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n5     5 <kmeans> <tibble [5 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n6     6 <kmeans> <tibble [6 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n```\n\n\n:::\n:::\n\n\nLists can sometimes be a bit tricky to get your head around, so it's worthwhile exploring the output. RStudio is particularly useful for this, since you can just left-click on the object in your `Environment` panel and look.\n\nThe way I see lists in this context is as containers. We have one huge table `kclusts` that contains all of the information that we need. Each 'cell' in this table has a container with the relevant data. The `kclust` column is a list with `kmeans` objects (the output of our `kmeans()` for each of the $k$ values), whereas the other columns are lists of tibbles (because the `tidy()`, `glance()` and `augment()` functions output a tibble with the information for each value of $k$).\n\nFor us to use the data in the lists, it makes sense to extract them on a column-by-column basis. We're ignoring the `kclust` column, because we don't need the actual `kmeans()` output any more.\n\nTo extract the data from the lists we use the `unnest()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclusters <- \n  kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  kclusts %>%\n  unnest(cols = c(glanced))\n```\n:::\n\n\nNext, we can visualise some of the data. We'll start by plotting the scaled data and colouring the data points based on the final cluster it has been assigned to by the `kmeans()` function.\n\nThe (augmented) data are in `assignments`. Have a look at the structure of the table.\n\nWe facet the data by $k$, so we get a single panel for each value of $k$.\n\nWe also add the calculated cluster centres, which are stored in `clusters`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(assignments,\n       aes(x = bill_depth_scaled,     # plot data\n           y = bill_length_scaled)) +  \n  geom_point(aes(color = .cluster),   # colour by cluster\n             alpha = 0.8) +           # add transparency\n  facet_wrap(vars(k)) +               # facet for each k\n  geom_point(data = clusters,         # add centers\n             size = 7,\n             shape = \"x\")\n```\n\n::: {.cell-output-display}\n![](mva-kmeans_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n:::\n\nLooking at this plot shows what we already knew (if only things were this easy all the time!): three clusters is a pretty good choice for these data. Remember that you're looking for clusters that are distinct, _i.e._ are separated from one another. For example, using `k = 4` gives you four nice groups, but two of them are directly adjacent, suggesting that they would do equally well in a single cluster.\n\n### Elbow plot\nVisualising the data like this can be helpful but at the same time it can also be a bit subjective (hoorah!). To find another subjective way of interpreting these clusters (remember, statistics isn't this YES/NO magic mushroom and we should be comfortable wandering around in the murky grey areas of statistics by now), we can plot the total within-cluster variation for each value of `k`.\n\nIntuitively, if you keep adding clusters then the total amount of variation that can be explained by these clusters will increase. The most extreme case would be where each data point is its own cluster and we can then explain all of the variation in the data.\n\nOf course that is not a very sensible approach - hence us balancing the number of clusters against how much variation they can capture.\n\nA practical approach to this is creating an \"elbow\" plot where the cumulative amount of variation explained is plotted against the number of clusters.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThe output of the `kmeans()` function includes `tot.withinss` - this is the total within-cluster sum of squares.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(clusterings,\n       aes(x = k,                # for each k plot...\n           y = tot.withinss)) +  # total within variance\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(\n    breaks = seq(1, 6, 1))       # set the x-axis breaks\n```\n\n::: {.cell-output-display}\n![](mva-kmeans_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n:::\n\nWe can see that the total within-cluster sum of squares decreases as the number of clusters increases. We can also see that from `k = 3` onwards the slope of the line becomes much shallower. This \"elbow\" or bending point is a useful gauge to find the optimum number of clusters.\n\nFrom the exploration we can see that three clusters are optimal in this scenario.\n\n\n## Exercises\n\n### Finch beaks {#sec-exr_finchkmeans}\n\n:::{.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nFor this exercise we'll be using the data from `data/finch_beaks.csv`.\n\nThe `finches` data has been adapted from the [accompanying website](https://doi.org/10.5061/dryad.g6g3h) to _40 years of evolution. Darwin's finches on Daphne Major Island_ by Peter R. Grant and Rosemary B. Grant.\n\nPractice running through the clustering workflow using the `finches` dataset. Try doing the following:\n\n1. Read in the data\n2. Explore and visualise the data\n3. Perform the clustering with `k = 2`\n4. Find out if using `k = 2` is a reasonable choice\n5. Try and draw some conclusions\n\n::: {.callout-answer collapse=\"true\"}\n\n#### Load and visualise\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinches <- read_csv(\"data/finch_beaks.csv\")\n\nhead(finches)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n   band species beak_length_mm beak_depth_mm  year\n  <dbl> <chr>            <dbl>         <dbl> <dbl>\n1     2 fortis             9.4           8    1975\n2     9 fortis             9.2           8.3  1975\n3    12 fortis             9.5           7.5  1975\n4    15 fortis             9.5           8    1975\n5   305 fortis            11.5           9.9  1975\n6   307 fortis            11.1           8.6  1975\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(finches, aes(x = beak_depth_mm,\n                    y = beak_length_mm,\n                    colour = species)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](mva-kmeans_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n:::\n\n#### Clustering\n\nNext, we perform the clustering. We first clean and scale the data.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinches_scaled <- finches %>% \n  select(beak_depth_mm,          # select data\n         beak_length_mm) %>% \n  drop_na() %>%                  # remove missing values\n  scale() %>%                    # scale the data\n  as_tibble() %>% \n  rename(bdepth_scaled = beak_depth_mm,\n         blength_scaled = beak_length_mm)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkclust <-\n  kmeans(finches_scaled,         # perform k-means clustering\n         centers = 2)            # using 2 centers\n\nsummary(kclust)                  # summarise output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Length Class  Mode   \ncluster      651    -none- numeric\ncenters        4    -none- numeric\ntotss          1    -none- numeric\nwithinss       2    -none- numeric\ntot.withinss   1    -none- numeric\nbetweenss      1    -none- numeric\nsize           2    -none- numeric\niter           1    -none- numeric\nifault         1    -none- numeric\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_clust <- tidy(kclust) # get centroid coordinates\n\ntidy_clust\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  bdepth_scaled blength_scaled  size withinss cluster\n          <dbl>          <dbl> <int>    <dbl> <fct>  \n1         0.618          0.676   339     486. 1      \n2        -0.672         -0.734   312     220. 2      \n```\n\n\n:::\n:::\n\n\n## Python\n\n:::\n\n#### Visualise the clusters\n\nWe can visualise the clusters as follows:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkclust %>%                              # take clustering data\n  augment(finches_scaled) %>%           # combine with original data\n  ggplot(aes(x = bdepth_scaled,         # plot the original data\n             y = blength_scaled)) +\n  geom_point(aes(colour = .cluster)) +  # colour by classification\n  geom_point(data = tidy_clust,\n             size = 7, shape = 'x')     # add the cluster centers\n```\n\n::: {.cell-output-display}\n![](mva-kmeans_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n:::\n\n#### Optimise clusters\n\nIt looks like two clusters is a reasonable choice. But let's explore this a bit more.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkclusts <- \n  # check for k = 1 to 6\n  tibble(k = 1:6) %>%\n  mutate(\n    # perform clustering for each k\n    kclust = map(k, ~ kmeans(finches_scaled, .x)),\n    # summary at per-cluster level\n    tidied = map(kclust, tidy),\n    # get single-row summary\n    glanced = map(kclust, glance),\n    # add classification to data set\n    augmented = map(kclust, augment, finches_scaled)\n  )\n```\n:::\n\n\nExtract the relevant data.\n\n::: {.cell}\n\n```{.r .cell-code}\nclusters <- \n  kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  kclusts %>%\n  unnest(cols = c(glanced))\n```\n:::\n\n\nVisualise the result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(assignments,\n       aes(x = bdepth_scaled,        # plot data\n           y = blength_scaled)) +  \n  geom_point(aes(color = .cluster),  # colour by cluster\n             alpha = 0.8) +          # add transparency\n  facet_wrap(~ k) +                  # facet for each k\n  geom_point(data = clusters,        # add centers\n             size = 7,\n             shape = 'x')\n```\n\n::: {.cell-output-display}\n![](mva-kmeans_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nCreate an elbow plot to have a closer look.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(clusterings,\n       aes(x = k,                # for each k plot...\n           y = tot.withinss)) +  # total within variance\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(\n    breaks = seq(1, 6, 1))       # set the x-axis breaks\n```\n\n::: {.cell-output-display}\n![](mva-kmeans_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n:::\n\n#### Conclusions\n\nOur initial clustering was done using two clusters, basically capturing the two different finch species.\n\nRedoing the analysis with different numbers of clusters seems to reasonably support that decision. The elbow plot suggests that `k = 3` would not be such a terrible idea either.\n\nIn the example above we used data that were collected at two different time points: 1975 and 2012.\n\nIn the analysis we've kept these data together. However, the original premises of these data was to see if there is any indication of evolution going on in these species of finches. Think about how you would approach this question!\n:::\n:::\n\n## Summary\n\n::: {.callout-tip}\n#### Key points\n\n- k-means clustering partitions data into clusters\n- the `k` defines the number of clusters\n- cluster centers or centroids get assigned randomly\n- each data point gets assigned to the closest centroid\n- the centroid of the new clusters gets calculated and the process of assignment and recalculation repeats until the cluster do no longer change\n- the optimal number of clusters can be determined with an 'elbow' plot\n:::\n",
    "supporting": [
      "mva-kmeans_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}