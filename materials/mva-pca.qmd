---
title: "Principal component analysis"
---

```{r}
#| echo: false
#| message: false
#| results: hide
source(file = "setup_files/setup.R")
```

```{python}
#| echo: false
#| message: false
import shutup;shutup.please()
exec(open('setup_files/setup.py').read())
```

::: {.callout-tip}
## Learning outcomes

- Understand when PCAs can be useful
- Be able to perform a PCA
- Learn how to plot and interpret a screeplot
- Plot and interpret the loadings for each PCA

:::

## Libraries and functions

::: {.callout-note collapse="true"}
## Click to expand

::: {.panel-tabset group="language"}
## R

### Libraries
### Functions

## Python

### Libraries
### Functions
:::
:::

## Purpose and aim

This is a statistical technique for reducing the dimensionality of a data set. The technique aims to find a new set of variables for describing the data. These new variables are made from a weighted sum of the old variables. The weighting is chosen so that the new variables can be ranked in terms of importance in that the first new variable is chosen to account for as much variation in the data as possible. Then the second new variable is chosen to account for as much of the remaining variation in the data as possible, and so on until there are as many new variables as old variables.


## Data

The example in this section uses the following data set:

`data/finches_hybridisation.csv`

These data come from an analysis of gene flow across two finch species [@grant2020]. They are slightly adapted here for illustrative purposes (the changes are documented in `materials/data_preparation.R` if you are interested).

Long story short: these data are part of a hybridisation study on finches at the Galapagos Islands. Here, we've only included the non-hybrid observations, but feel free to rerun the analysis with all the hybrids!

From many years of studies, going back to Darwin, we now know that the beak shape of the finches are important in their chances for survival. Changes in beak shape have led to new species and this study explored how movement of genes from a rare immigrant species (*Geospiza fuliginosa*) occurred through two other species (*G. fortis* and *G. scandens*). The researchers recorded various morphological traits.


It uses the following abbreviations:

* `F`	= *G. fortis*
* `f`	= *G. fuliginosa*
* `S`	= *G. scandens*

## Load and visualise the data

First, we load the data:

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
#| warning: false
finches_hybrid <- read_csv("data/finches_hybridisation.csv")
```

## Python

```{python}
finches_hybrid_py = pd.read_csv("data/finches_hybridisation.csv")
```

:::

Next, we visualise the data:

::: {.panel-tabset group="language"}
## R

```{r}
head(finches_hybrid)
```

## Python

```{python}
finches_hybrid_py.head()
```
:::

We have `r finches_hybrid %>% ncol()` columns. We're not going to recreate the published analysis exactly, but for the purpose of this section we will be looking if differences in the measured morphological traits can be attributed to specific categories.

We have `r finches_hybrid %>% distinct(category) %>% nrow()` different `category` values:

::: {.panel-tabset group="language"}
## R

```{r}
finches_hybrid %>% distinct(category)
```

## Python

```{python}
finches_hybrid_py['category'].unique()
```

:::

We have 6 continuous response variables, which we're not going to visualise independently! However, you *could* look at some of the potential relationships within the data.

For example, looking at body weight (`weight`) against beak length (`blength`):

::: {.panel-tabset group="language"}
## R

```{r}
ggplot(finches_hybrid, aes(x = blength, y = weight,
                           colour = category)) +
         geom_point()
```

or just `weight` across the different categories:

```{r}
ggplot(finches_hybrid, aes(x = category, y = weight)) +
  geom_boxplot()
```

## Python

```{python}
#| results: hide
(ggplot(finches_hybrid_py,
         aes(x = "blength", y = "weight",
             colour = "category")) +
     geom_point())
```

or just `weight` across the different categories:

```{python}
#| results: hide
(ggplot(finches_hybrid_py,
         aes(x = "category", y = "weight")) +
     geom_boxplot())
```


:::

The number of combinations are rather large, given the number of variables we have. Hence it's a good idea to see if we can "condense" some of variables into new ones.

What we're doing with a PCA is trying to capture as much *variance* that exists in the data using a Principal Component (PC). The PC therefore explains some of the variance for each of the individual variables.

I like to compare the PCs to a smoothie: a smoothie might consist of 4 parts apple, 3 parts strawberry, 2 parts blueberry and 1 part grape. Along the lines of that delicious metaphor, one of our Principal components may consist of 4 parts `blength`, 3 parts `weight`, 2 parts `bdepth` and 1 part `wing`. We don't know this yet, so let's go find out.

## Performing the PCA

To perform a PCA, we need to keep a few things in mind:

1. We can only calculate principal components for numeric data
2. The numeric data need to be on the same **scale**

This last point of scaling is very important. Measurements can take place at different scales. You shouldn't compare milimetres and kilogrammes directly, for example. Or seconds and height. That simply does not make sense.

This issue is even more prevalent when comparing gene expression data, for example. Genes can be active at varying levels, but some genes only need (an indeed *do*) change a tiny amount to elicit an effect, whereas other genes might have to change several fold-changes before something happens.

To compensate for this, we bring all of our data onto the same scale.

::: {.panel-tabset group="language"}
## R

In R we can scale our data with the `scale()` function. We perform the PCA using the `prcomp()` function. Here we store the output into an object called `pca_fit`, because we'll be working with the output later on.

```{r}
pca_fit <- finches_hybrid %>% 
  # keep only the numeric columns
  select(where(is.numeric)) %>%
  # scale the data
  scale() %>%
  # perform the PCA
  prcomp()
```

This is a bit of daunting looking output, but not to worry. We'll unpack things along the way!

```{r}
pca_fit
```


## Python

In Python we can scale our data with the `StandardScaler()` function from `sklearn.preprocessing`. We can only scale numerical data, so we need to get those first.

```{python}
from sklearn.preprocessing import StandardScaler

# select the numeric values
X = finches_hybrid_py.select_dtypes(include = ['float64', 'int64'])

# define the scaler
std_scaler = StandardScaler()

# scale the numeric values
finches_scaled = std_scaler.fit_transform(X)
```

Now that we have the scaled values, we can perform the PCA. We do this using the `PCA()` function from `sklearn.decomposition`.

We need to tell it how many principal components we want it to return. We set it to `r finches_hybrid %>% ncol() - 1` here, which is the number of numerical variables.

```{python}
from sklearn.decomposition import PCA

# define the number of principal components
n_components = 6

# set the number of principal components
pca = PCA(n_components = n_components)

# perform the PCA
principal_components = pca.fit_transform(finches_scaled)

# create a data frame containing the information
# changing the column names based on the PC
pca_fit_py = pd.DataFrame(data = principal_components, columns=[f'PC{i}' for i in range(1, n_components + 1)])
```


:::

### Visualising the principal components

We can figure out how much each principal component is contributing to the amount of variance that is being explained. This is called a *screeplot*.

::: {.panel-tabset group="language"}
## R

```{r}
pca_fit %>% 
  tidy(matrix = "eigenvalues") %>% 
  ggplot(aes(x = PC,
             y = percent)) +
  geom_point() +
  geom_line()
```

## Python

First, we extract the amount of variance explained by each principal component. Next, we convert this to a DataFrame:

```{python}
explained_variance_pc = pca.explained_variance_ratio_

percent = (
  pd.DataFrame({'variance_explained':
                (explained_variance_pc * 100),
                'PC': [f'PC{i+1}' for i in range(n_components)]})
                )
                
percent.head()
```

Next, we can plot this:

```{python}
#| results: hide
(ggplot(percent,
         aes(x = "PC", y = "variance_explained")) +
     geom_point() +
     geom_line(group = 1))
```


:::

This means that PC1 is able to explain around `r round(pca_fit %>% tidy(matrix = "eigenvalues") %>% slice(1) %>% pull(percent) * 100, digits = 0)`% of the variance in the data. PC2 is able to explain around `r round(pca_fit %>% tidy(matrix = "eigenvalues") %>% slice(2) %>% pull(percent) * 100, digits = 0)`% of the variance in the data, and so forth.

### Loadings

Let's think back to our smoothy metaphor. Remember how the smoothy was made up of various fruits - just like our PCs are made up of parts of our original variables.

Let's, for the sake of illustrating this, assume the following for PC1:

| parts| variable|
|:- |:- |
| 4 | `blength` |
| 1 | `tarsus` |

Each PC has something called an **eigenvector**, which in simplest terms is a line with a certain direction and length.

If we want to calculate the length of the eigenvector for PC1, we can employ Pythagoras (well, not directly, just his legacy). This gives:

$eigenvector \, PC1 = \sqrt{4^2 + 1^2} = 4.12$

The **loading scores** for PC1 are the "parts" scaled for this length, _i.e._:

| scaled parts| variable|
|:- |:- |
| 4 / 4.12 = 0.97 | `blength` |
| 1 / 4.12 = 0.24 | `tarsus` |

What we can do with these values is plot the **loadings** for each of the original variables.

::: {.panel-tabset group="language"}
## R

It might be helpful to visualise this in context of the original data. We can easily add the original data to the fitted PCs as follows (and plot it):

```{r}
pca_fit %>%
  # add the original data
  augment(finches_hybrid) %>%
  ggplot(aes(.fittedPC1, .fittedPC2, colour = category)) + 
  geom_point(size = 1.5)
```

This gives us the individual contributions to PC1 and PC2 for each observation.

If we wanted to know how much each *variable* is contributing to PC1 and PC2 we would have to use the loadings.

We can extract all the loadings as follows:

```{r}
pca_fit %>% 
  tidy(matrix = "loadings")
```

We'll have to reformat this a little, so that we have the values in separate columns. First, we rename some of the columns / values to make things more consistent and clearer:

```{r}
pca_loadings <- pca_fit %>% 
  tidy(matrix = "loadings") %>% 
  rename(terms = column,
         component = PC) %>% 
  mutate(component = paste0("PC", component))

head(pca_loadings)
```

Now we can reformat the data:

```{r}
pca_loadings <- pca_loadings %>% 
  pivot_wider(names_from = "component",
              values_from = "value")

pca_loadings
```

We can then plot this. This is a little bit involved, unfortunately. And not something I'd recommend remembering the code by heart, but we're doing the following:

1. Take the PCA output and add the original data
2. Plot this
3. Create a line from the origin (`x = 0`, `y = 0`) for each loading
4. Make it an arrow
5. Add the variable name as a label

We define the arrow as follows:

```{r}
# define arrow style
arrow_style <- arrow(length = unit(2, "mm"),
                     type = "closed")
```


```{r}
pca_fit %>%
  # add the original data
  augment(finches_hybrid) %>%
  ggplot() + 
  geom_point(aes(.fittedPC1, .fittedPC2, colour = category), size = 1.5) +
  geom_segment(data = pca_loadings,
               aes(xend = PC1, yend = PC2),
               x = 0, y = 0,
               arrow = arrow_style) +
  geom_text(data = pca_loadings,
            aes(x = PC1, y = PC2, label = terms), 
            hjust = 0, 
            vjust = 1,
            size = 5) 
```


## Python

:::

After all that we end up with a rather unclear plot. The `r pca_loadings %>% nrow()` variables that contribute to each principal component have very overlapping contributions in PC1. As such, it's difficult to see which variable contributes what!

The reason why I'm still showing it here is that this kind of plot is very often used in PCA, so at least you can recognise it. If the variables have well-separated contributions across the two principal components, then it can be quite informative.

A better way would be to plot the individual contributions to each principal component as an ordered bar plot. This does require some rejigging of the data again (sorry!).

::: {.panel-tabset group="language"}
## R

First, we convert our loadings data back to a "long" format. We also add an extra column, `direction`, which indicates if the contribution to the principal component is positive or negative.

```{r}
pca_loadings <- pca_loadings %>% 
  pivot_longer(cols = -terms,
               names_to = "component",
               values_to = "value") %>% 
  # add a column that indicates direction
  # (positive or negative)
  mutate(direction = ifelse(value < 0, "positive", "negative"))

# have a look at the data
head(pca_loadings)
```

We can now visualise this. Here we are using some additional functionality offered by the `tidytext` library. Make sure to install it, if needed. Then load it.

```{r}
# we need this library
library(tidytext)

pca_loadings %>% 
  mutate(terms = tidytext::reorder_within(terms,
                                          abs(value),
                                          component)) %>%
  ggplot(aes(x = abs(value), y = terms, fill = direction)) +
  geom_col() +
  facet_wrap(vars(component), scales = "free_y") +
  tidytext::scale_y_reordered()
```

## Python

:::

:::{.callout-important}
It is important to keep the amount of variance explained by each principal component in mind. For example, PC3 only explains around `r round(pca_fit %>% tidy(matrix = "eigenvalues") %>% filter(PC == 3) %>% pull(percent) * 100, digits = 1)` of the variance. So although several variables contribute substantially to PC3, the contribution of PC3 itself remains small.
:::

## Exercises

### Penguins {#sec-exr_penguins}

:::{.callout-exercise}

{{< level 2 >}}

For this exercise we'll be using the data from `data/penguins.csv`. These data are from the `palmerpenguins` package (for more information, see [the GitHub page](https://github.com/allisonhorst/palmerpenguins)).

I would like you to do the following:

1. Load and visualise the data.
2. Create a screeplot and see how many PCs would be best.
3. Calculate the loadings for PC1 and PC2 and visualise them.
4. Any conclusions you might draw from the analysis.

::: {.callout-answer collapse="true"}

#### Load and visualise the data

::: {.panel-tabset group="language"}
## R

```{r}
penguins <- read_csv("data/penguins.csv")
```

```{r}
head(penguins)
```

We can see that there are different kinds of variables, both categorical and numerical. Also, there appear to be some missing data in the data set, so we probably have to deal with that.

Lastly, we should be careful with the `year` column: it is recognised as a numerical column (because it contains numbers), but we should view it as a factor, since the years have an ordered, categorical meaning.

To get a better sense of our data we could plot all the numerical variables against each other, to see if there is any possible correlation between them. Doing that one-by-one in ggplot is tedious, so I'm using the `pairs()` function here. Pretty? No. Effective? Yes.

```{r}
penguins %>% 
  select(where(is.numeric)) %>% 
  pairs()
```



## Python

:::

So we see that there is some possible grouping going on and possibly some linear relationships, too. However, there are multiple groups and closely related measurements, so it is not very surprising that there are possible relationships within the data.

#### Perform the PCA

First, we need to do a little bit of data tidying. We convert `year` to a factor and deal with the missing values. We're not dealing with them in a particularly subtle way here, removing all the rows that contain at least one missing value.

In your own research you may want to be more careful and only remove missing values from variables that you using in the PCA (here we include everything).

::: {.panel-tabset group="language"}
## R

```{r}
penguins <- penguins %>% 
  mutate(year = factor(year)) %>% 
  drop_na() # remove all rows with missing values
```

```{r}
pca_fit <- penguins %>% 
  # keep only the numeric columns
  select(where(is.numeric)) %>%
  # scale the data
  scale() %>%
  # perform the PCA
  prcomp()
```

## Python

:::

#### Visualise the PCs

::: {.panel-tabset group="language"}
## R

We can visualise the principal components:

```{r}
pca_fit %>% 
  tidy(matrix = "eigenvalues") %>% 
  ggplot(aes(x = PC,
             y = percent)) +
  geom_point() +
  geom_line()
```


## Python

:::

It looks like using the first two principal components is probably capturing the majority of the variance in the data. Combined they account for `r round(pca_fit %>% tidy(matrix = "eigenvalues") %>% filter(PC %in% c(1, 2)) %>% pull(percent) %>% sum(), 1) * 100`% of the variance.

#### Loadings

Next, we get the loadings: how much is each original variable contributing to the individual principal components?

::: {.panel-tabset group="language"}
## R

We start with some data tidying and wrangling, since we need the data in a "wide" format for the next plot to work:

```{r}
pca_loadings <- pca_fit %>% 
  tidy(matrix = "loadings") %>% 
  rename(terms = column,
         component = PC) %>% 
  mutate(component = paste0("PC", component)) %>% 
  pivot_wider(names_from = "component",
              values_from = "value")

head(pca_loadings)
```

Arrow style:

```{r}
# define arrow style
arrow_style <- arrow(length = unit(2, "mm"),
                     type = "closed")
```

```{r}
pca_fit %>%
  # add the original data
  augment(penguins) %>%
  ggplot() + 
  geom_point(aes(.fittedPC1, .fittedPC2, colour = species), size = 1.5) +
  geom_segment(data = pca_loadings,
               aes(xend = PC1, yend = PC2),
               x = 0, y = 0,
               arrow = arrow_style) +
  geom_text(data = pca_loadings,
            aes(x = PC1, y = PC2, label = terms), 
            hjust = 0, 
            vjust = 1,
            size = 5) 
```

We can also visualise these contributions using a bar plot. We need the data in a slightly different format before we can do this:

```{r}
pca_loadings <- pca_loadings %>% 
  pivot_longer(cols = -terms,
               names_to = "component",
               values_to = "value") %>% 
  # add a column that indicates direction
  # (positive or negative)
  mutate(direction = ifelse(value < 0, "positive", "negative"))

# have a look at the data
head(pca_loadings)
```

But after that, we can visualise it as follows:

```{r}
# we need this library
library(tidytext)

pca_loadings %>% 
  mutate(terms = tidytext::reorder_within(terms,
                                          abs(value),
                                          component)) %>%
  ggplot(aes(x = abs(value), y = terms, fill = direction)) +
  geom_col() +
  facet_wrap(vars(component), scales = "free_y") +
  tidytext::scale_y_reordered()
```

## Python

:::

#### Conclusions
1. Load the data.
2. Create a screeplot and see how many PCs would be best.
3. Calculate the loadings for PC1 and PC2 and visualise them.
4. Any conclusions you might draw from the analysis.

Taken together, we can conclude/comment on a few things:

1. Endlessly looking at pairwise comparisons between continuous variables probably becomes a bit tedious. An alternative would be to calculate correlations between the variables to get some insight into your data. In the end it depends on how many variables you have and how much you (want to) know about them.
2. In this case, I'd say that the first two principal components capture most of the variance in the data.
3. The largest contributing variables mostly differ between the first two principal components. The variables that make up PC1 are very similar in terms of contribution, whereas two variables more or less make up PC2 entirely.

The variables `flipper_length_mm` and `body_mass_g` contribute pretty much only to PC1 (they are horizontal in the loadings plot), whereas `bill_length_mm` is contributing a reasonable amount to both PC1 *and* PC2.

From the data itself we can see that there is clear separation between the three `species`. The Gentoo penguins separate further once again from the other two species.

:::
:::

## Summary

::: {.callout-tip}
#### Key points

- PCA allows you to reduce a large number of variables into fewer principal components.
- Each PC is made up of a combination of the original variables and captures as much of the variance within the data as possible.
- The loadings tell you how much each original variable contributes to each PC.
- A screeplot is a graphical representation of the amount of variance explained by each PC.
:::
