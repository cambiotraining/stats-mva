---
title: "Principal component analysis"
---

```{r}
#| echo: false
#| message: false
#| results: hide
source(file = "setup_files/setup.R")
```

```{python}
#| echo: false
#| message: false
import shutup;shutup.please()
exec(open('setup_files/setup.py').read())
```

::: {.callout-tip}
## Learning outcomes

- Understand when PCAs can be useful
- Be able to perform a PCA
- Learn how to plot and interpret a screeplot
- Plot and interpret the loadings for each PCA

:::

## Libraries and functions

::: {.callout-note collapse="true"}
## Click to expand

::: {.panel-tabset group="language"}
## R

### Libraries
### Functions

## Python

### Libraries
### Functions
:::
:::

## Purpose and aim

This is a statistical technique for reducing the dimensionality of a data set. The technique aims to find a new set of variables for describing the data. These new variables are made from a weighted sum of the old variables. The weighting is chosen so that the new variables can be ranked in terms of importance in that the first new variable is chosen to account for as much variation in the data as possible. Then the second new variable is chosen to account for as much of the remaining variation in the data as possible, and so on until there are as many new variables as old variables.


## Data

The example in this section uses the following data set:

`data/finches_hybridisation.csv`

These data come from an analysis of gene flow across two finch species [@grant2020]. They are slightly adapted here for illustrative purposes (the changes are documented in `materials/data_preparation.R` if you are interested).

Long story short: these data are part of a hybridisation study on finches at the Galapagos Islands. Here, we've only included the non-hybrid observations, but feel free to rerun the analysis with all the hybrids!

From many years of studies, going back to Darwin, we now know that the beak shape of the finches are important in their chances for survival. Changes in beak shape have led to new species and this study explored how movement of genes from a rare immigrant species (*Geospiza fuliginosa*) occurred through two other species (*G. fortis* and *G. scandens*). The researchers recorded various morphological traits.


It uses the following abbreviations:

* `F`	= *G. fortis*
* `f`	= *G. fuliginosa*
* `S`	= *G. scandens*

## Load and visualise the data

First, we load the data:

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
#| warning: false
finches_hybrid <- read_csv("data/finches_hybridisation.csv")
```

## Python

```{python}
finches_hybrid_py = pd.read_csv("data/finches_hybridisation.csv")
```

:::

Next, we visualise the data:

::: {.panel-tabset group="language"}
## R

```{r}
head(finches_hybrid)
```

## Python

```{python}
finches_hybrid_py.head()
```
:::

We have `r finches_hybrid %>% ncol()` columns. We're not going to recreate the published analysis exactly, but for the purpose of this section we will be looking if differences in the measured morphological traits can be attributed to specific categories.

We have `r finches_hybrid %>% distinct(category) %>% nrow()` different `category` values:

::: {.panel-tabset group="language"}
## R

```{r}
finches_hybrid %>% distinct(category)
```

## Python

```{python}
finches_hybrid_py['category'].unique()
```

:::

We have 6 continuous response variables, which we're not going to visualise independently! However, you *could* look at some of the potential relationships within the data.

For example, looking at body weight (`weight`) against beak length (`blength`):

::: {.panel-tabset group="language"}
## R

```{r}
ggplot(finches_hybrid, aes(x = blength, y = weight,
                           colour = category)) +
         geom_point()
```

or just `weight` across the different categories:

```{r}
ggplot(finches_hybrid, aes(x = category, y = weight)) +
  geom_boxplot()
```

## Python

```{python}
#| results: hide
(ggplot(finches_hybrid_py,
         aes(x = "blength", y = "weight",
             colour = "category")) +
     geom_point())
```

or just `weight` across the different categories:

```{python}
#| results: hide
(ggplot(finches_hybrid_py,
         aes(x = "category", y = "weight")) +
     geom_boxplot())
```


:::

The number of combinations are rather large, given the number of variables we have. Hence it's a good idea to see if we can "condense" some of variables into new ones.

What we're doing with a PCA is trying to capture as much *variance* that exists in the data using a Principal Component (PC). The PC therefore explains some of the variance for each of the individual variables.

I like to compare the PCs to a smoothie: a smoothie might consist of 4 parts apple, 3 parts strawberry, 2 parts blueberry and 1 part grape. Along the lines of that delicious metaphor, one of our Principal components may consist of 4 parts `blength`, 3 parts `weight`, 2 parts `bdepth` and 1 part `wing`. We don't know this yet, so let's go find out.

## Performing the PCA

To perform a PCA, we need to keep a few things in mind:

1. We can only calculate principal components for numeric data
2. The numeric data need to be on the same **scale**

This last point of scaling is very important. Measurements can take place at different scales. You shouldn't compare milimetres and kilogrammes directly, for example. Or seconds and height. That simply does not make sense.

This issue is even more prevalent when comparing gene expression data, for example. Genes can be active at varying levels, but some genes only need (an indeed *do*) change a tiny amount to elicit an effect, whereas other genes might have to change several fold-changes before something happens.

To compensate for this, we bring all of our data onto the same scale.

::: {.panel-tabset group="language"}
## R

In R we can scale our data with the `scale()` function. We perform the PCA using the `prcomp()` function. Here we store the output into an object called `pca_fit`, because we'll be working with the output later on.

```{r}
pca_fit <- finches_hybrid %>% 
  # keep only the numeric columns
  select(where(is.numeric)) %>%
  # scale the data
  scale() %>%
  # perform the PCA
  prcomp()
```

This is a bit of daunting looking output, but not to worry. We'll unpack things along the way!

```{r}
pca_fit
```


```{r}
pca_fit %>%
  # add the original data
  augment(finches_hybrid) %>%
  ggplot(aes(.fittedPC1, .fittedPC2, colour = category)) + 
  geom_point(size = 1.5)
```


## Python

In Python we can scale our data with the `StandardScaler()` function from `sklearn.preprocessing`. We can only scale numerical data, so we need to get those first.

```{python}
from sklearn.preprocessing import StandardScaler

# select the numeric values
X = finches_hybrid_py.select_dtypes(include = ['float64', 'int64'])

# define the scaler
std_scaler = StandardScaler()

# scale the numeric values
finches_scaled = std_scaler.fit_transform(X)
```

Now that we have the scaled values, we can perform the PCA. We do this using the `PCA()` function from `sklearn.decomposition`.

We need to tell it how many principal components we want it to return. We set it to `r finches_hybrid %>% ncol() - 1` here, which is the number of numerical variables.

```{python}
from sklearn.decomposition import PCA

# define the number of principal components
n_components = 6

# set the number of principal components
pca = PCA(n_components = n_components)

# perform the PCA
principal_components = pca.fit_transform(finches_scaled)

# create a data frame containing the information
# changing the column names based on the PC
pca_fit_py = pd.DataFrame(data = principal_components, columns=[f'PC{i}' for i in range(1, n_components + 1)])
```


:::

We can figure out how much each principal component is contributing to the amount of variance that is being explained:

::: {.panel-tabset group="language"}
## R

```{r}
pca_fit %>% 
  tidy(matrix = "eigenvalues") %>% 
  ggplot(aes(x = PC,
             y = percent)) +
  geom_point() +
  geom_line()
```

## Python

First, we extract the amount of variance explained by each principal component. Next, we convert this to a DataFrame:

```{python}
explained_variance_pc = pca.explained_variance_ratio_

percent = (
  pd.DataFrame({'variance_explained':
                (explained_variance_pc * 100),
                'PC': [f'PC{i+1}' for i in range(n_components)]})
                )
                
percent.head()
```

Next, we can plot this:

```{python}
#| results: hide
(ggplot(percent,
         aes(x = "PC", y = "variance_explained")) +
     geom_point() +
     geom_line(group = 1))
```


:::

This means that PC1 is able to explain around `r round(pca_fit %>% tidy(matrix = "eigenvalues") %>% slice(1) %>% pull(percent) * 100, digits = 0)`% of the variance in the data. PC2 is able to explain around `r round(pca_fit %>% tidy(matrix = "eigenvalues") %>% slice(2) %>% pull(percent) * 100, digits = 0)`% of the variance in the data, and so forth.

::: {.panel-tabset group="language"}
## R

## Python

:::

::: {.panel-tabset group="language"}
## R

## Python

:::



## Exercises

### Title {#sec-exr_title}

:::{.callout-exercise}

{{< level 2 >}}

For this exercise we'll be using the data from `data/file.csv`.

::: {.callout-answer collapse="true"}

::: {.panel-tabset group="language"}
## R

## Python

:::
:::
:::

## Summary

::: {.callout-tip}
#### Key points

- PCA allows you to reduce a large number of variables into fewer principal components
- Each PC is made up of a combination of the original variables and captures as much of the variance within the data as possible
- The loadings tell you how much each original variable contributes to each PC
- A screeplot is a graphical representation of the amount of variance explained by each PC
:::
